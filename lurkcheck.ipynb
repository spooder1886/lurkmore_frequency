import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from natasha import MorphVocab
import os

nltk.download('stopwords')
stop_words = set(stopwords.words('russian'))

lurkfiles_path = r'C:\Users\spood\Desktop\курсач 3 курс\giza\lurkfiles'
full = []

for file in os.listdir(lurkfiles_path):
    file_path = os.path.join(lurkfiles_path, file)
    data = pd.read_csv(file_path)
    original_column = data['original']
    original_list = list(original_column)
    full.extend(original_list)

tokenizer = RegexpTokenizer(r'\w+')
lower_lst = [tokenizer.tokenize(sentence.lower()) for sentence in full]

filtered_lst = [word for sentence in lower_lst for word in sentence if word not in stop_words]

morph_vocab = MorphVocab()

lemmatized_lst = []
for word in filtered_lst:
    doc = Doc(word)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    doc.lemmatize(morph_vocab)
    lemmas = [token.lemma for token in doc.tokens]
    lemmatized_lst.extend(lemmas)

word_counts = {}
for word in lemmatized_lst:
    word_counts[word] = word_counts.get(word, 0) + 1

sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

for word, count in sorted_words:
    if word not in stop_words:
        print(f'{word}: {count}')
